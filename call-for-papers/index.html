<!DOCTYPE html>
<html lang="en"><head>

    <meta name="generator" content="Hugo 0.147.6">
    <meta name="date" content="2025-05-27T11:17:16Z">
    
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    
    <meta name="author" content=" TODO   " />
    <meta name="description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    <meta name="keywords" content="workshop, AI interpretability, cognitive science" />
    
    <title>CogInterp 2025 | Call for Papers</title>
    
    <meta property="og:title" content="Call for Papers" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    
    <meta name="twitter:title" content="" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> 
    
    <link rel="canonical" href="https://coginterp.github.io/neurips2025/call-for-papers/">
    <link rel="stylesheet" href="https://coginterp.github.io/neurips2025/styles.css">
    
    <link rel="apple-touch-icon" sizes="180x180" href="https://coginterp.github.io/neurips2025/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://coginterp.github.io/neurips2025/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://coginterp.github.io/neurips2025/favicon-16x16.png">
    <link rel="manifest" href="https://coginterp.github.io/neurips2025/site.webmanifest">

</head>
<body><section id="header">

    <div id="logo-container">
        <div id="title-inner-container">
            <a href="https://coginterp.github.io/neurips2025/"><img src="https://coginterp.github.io/neurips2025/logo.png" width="60%" id="logo"></a>
        </div>
    </div>

    <div id="title-container">
        <div id="title-inner-container">
            <div class="supertitle">First Workshop on</div>
            <div class="title"><a href="https://coginterp.github.io/neurips2025/"><b>CogInterp</b>: Interpreting Cognition <br> in Deep Learning Models</a></div>
            <div class="subtitle">Dec. 6 or 7 @ NeurIPS 2025</div>
        </div>
    </div>

    <br>

    <div id="navigation">
        <ul>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/">About</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/call-for-papers">Call for Papers</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/schedule">Schedule</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/speakers">Speakers</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/organizers-reviewers">Organizers</a></strong></li>
        
      </ul>
    </div>

</section><section id="content">
        
    <h1 id="call-for-papers">Call for Papers</h1>
<p>We welcome submissions related to any aspects of multimodal representation learning, including but not limited to:</p>
<ul>
<li>Properties of multimodal representations.</li>
<li>Insights on interactions across modalities.</li>
<li>Novel applications regarding the nature and number of modalities.</li>
</ul>
<p>In particular, we encourage submission that address the following questions:</p>
<ul>
<li><strong>Representation:</strong> How do we identify useful properties of multimodal representations?
<ul>
<li>What semantic information is encoded in the learned representations?</li>
<li>How does the geometry of the representation space affect the quality of the learned representations?</li>
<li>What properties are leveraged for downstream tasks?</li>
</ul>
</li>
<li><strong>Training:</strong> How can we promote useful properties of multimodal representations?
<ul>
<li>What are the limits of representation models, in regard to the number of modalities?</li>
<li>How do different learning objectives influence the resulting representations?</li>
<li>How do we promote the robustness of the representations to adversarial attacks, missing input modalities, and noise?</li>
</ul>
</li>
<li><strong>Modalities:</strong> What makes a modality different? How can we improve their interactions?
<ul>
<li>How can we quantify the (dis)similarity between modalities?</li>
<li>How do different modalities contribute to the semantics of the learned representations?</li>
<li>What are the representation benefits of having multimodal observations as opposed to just a single modality?</li>
</ul>
</li>
</ul>
<p>The MRL workshop has the objective to bring together experts from the multimodal learning community in order to advance these fundamental questions and discuss the future of the field. We invite submissions that present analysis of the properties of multimodal representations, insights on interactions across modalities, as well as novel applications regarding the nature and number of modalities employed.</p>
<h2 id="submission">Submission</h2>
<p>The submission will be open on <a href="https://openreview.net/group?id=ICLR.cc/2023/Workshop/MRL">OpenReview</a> between December 20 and January 30, 2023, AoE. For all relevant dates, please see <a href="https://coginterp.github.io/neurips2025/#dates">Dates</a>. The formatting instructions are provided below.</p>
<p>Please note that it will be requested that at least one author of each submission participates in reviewing for the workshop.</p>
<p>We will not accept submissions that have already been accepted for publication in other venues with archival proceedings (including publications that will be presented at the main ICLR conference). We discourage dual submission to concurrent ICLR workshops.</p>
<h2 id="code-of-ethics-and-conduct">Code of Ethics and Conduct</h2>
<p>All participants of the workshop (including authors and reviewers) are required to adhere to the <a href="https://iclr.cc/public/CodeOfEthics">ICLR Code of Ethics</a> and <a href="https://iclr.cc/public/CodeOfConduct">ICLR Code of Conduct</a>.</p>
<hr />
<h1 id="formatting-instructions">Formatting Instructions</h1>
<h2 id="style--author-instructions">Style &amp; Author Instructions</h2>
<p>Submissions should be formatted using the <a href="https://github.com/ICLR/Master-Template/raw/master/iclr2023.zip">ICLR 2023 latex template and formatting instructions</a>. Papers must be submitted as a PDF file and there will be a strict upper limit of 4 pages for the main text, which should include all main results, figures, and tables. This page limit applies to both the initial and final camera-ready version, including all main results, figures, and tables. There is no page limit for the citations, and additional appendices for supplementary details are allowed, but reviewers are not expected to take the appendices into account.</p>
<h2 id="camera-ready-revisions">Camera-Ready Revisions</h2>
<p>Camera-ready revisions will be possible through OpenReview. While the workshop has no official proceedings (papers will be publicly available as <em>non-archival</em> reports through OpenReview), we strongly encourage authors to submit a revised &ldquo;camera-ready&rdquo; version taking reviewers&rsquo; comments and suggestions into account. We suggest uploading a revised version prior to the workshop, and possibly another final version (incorporating additional feedback from the poster session and workshop) one week after the workshop.</p>


    </section>
<div id="footer">
    Made with <a href="https://gohugo.io/">Hugo</a> and hosted on <a href="https://github.com/coginterp/neurips2025">GitHub</a>.
</div>


</body>

</html>