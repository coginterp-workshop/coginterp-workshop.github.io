---
title: "Home"
---

# Welcome!

Welcome to the first workshop on **Interpreting Cognition in Deep Learning Models**. If accepted, the workshop would take place at [**NeurIPS 2025**](https://neurips.cc/Conferences/2025) in San Diego, USA on December 6 or 7, 2025.

<!-- <div  style="text-align: center"><span class="alert"> We have extended the deadline until February 3, 2023, 23:59 AoE.</span></div> -->


<!-- <span class="alert">Most information is still preliminary and could change in the near future.</span> -->


## About the workshop

Recent innovations in deep learning have produced models with impressive capabilities, achieving or even
exceeding human performance in a wide range of domains. A timely and critical challenge in
AI is understanding what behaviors these models are actually capable of, and the internal “mental” processes
which support these behaviors. As interest continues to grow in models’ internal processes, the field of cognitive science is becoming increasingly useful for describing and understanding cognition in deep learning better rmodels: cognitive science, which seeks to describe the cognitive processes in human and animal minds, offers a rich body of theories, experiments, and frameworks which may be adopted to understand how deep learning models achieve complex behaviors in domains such as language, vision, and reasoning. 

The proposed workshop will focus on Cognitive Interpretability (“CogInterp”), which involves the systematic interpretation of high-level cognition in deep learning models. Similar to how cognitive science describes the intermediate representations and algorithms (or cognition) between behavior and neurons in biological systems, Cognitive Interpretability describes the cognitive processes which lie between the levels of behavioral evaluations and mechanistic interpretability in deep learning models. Practically speaking, this means that Cognitive Interpretability does not just ask whether a model can perform task X or has a certain ability Y , but additionally (or instead) how a model performs X or learns and implements Y . These kinds of inferences—from observable behavior to latent mental processes—are the bread and butter of cognitive science, but many of the theoretical and empirical tools developed to tackle these problems have not yet been widely adopted in AI research, in part because of the separation between the fields and communities. 

To address this gap, our goal is to bring together researchers in cognitive science and AI interpretability to discuss new empirical results and theories about the inner workings of deep learning models. We hope to gather perspectives from various disciplines, including machine learning, psychology, linguistics, vision science, neuroscience, philosophy of mind, and law.


## Important Dates

<div id="dates" style="margin-bottom: 1em"></div>
<!-- <span class="alert">Preliminary dates, they are subject to change.</span>  -->

* **Paper submission start:** July 5, 2025  (midnight AoE)
* **Paper submission deadline**: August 22, 2025  (midnight AoE)
* **Notification to authors:** September 22, 2025  (midnight AoE)
* **Camera-ready version:** TBA
* **Workshop date:** December 6 or 7, 2025

Contact us at <coginterp@gmail.com>. Specific dates are subject to change. 



<!-- ## With support from

  <div id="sponsor-logo-container">
      <div id="sponsor-inner-container">
          <img src="/googlelogo_color_416x140dp.png" width="25%" id="sponsor-logo">
      </div>
  </div>
 -->