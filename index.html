<!DOCTYPE html>
<html lang="en"><head>

    <meta name="generator" content="Hugo 0.147.6">
    <meta name="date" content="2025-05-27T11:17:16Z">
    
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    
    <meta name="author" content=" TODO   " />
    <meta name="description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    <meta name="keywords" content="workshop, AI interpretability, cognitive science" />
    
    <title>CogInterp 2025 | Home</title>
    
    <meta property="og:title" content="Home" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    
    <meta name="twitter:title" content="" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> 
    
    <link rel="canonical" href="https://coginterp.github.io/neurips2025/">
    <link rel="stylesheet" href="https://coginterp.github.io/neurips2025/styles.css">
    
    <link rel="apple-touch-icon" sizes="180x180" href="https://coginterp.github.io/neurips2025/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://coginterp.github.io/neurips2025/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://coginterp.github.io/neurips2025/favicon-16x16.png">
    <link rel="manifest" href="https://coginterp.github.io/neurips2025/site.webmanifest">

</head>
<body><section id="header">

    <div id="logo-container">
        <div id="title-inner-container">
            <a href="https://coginterp.github.io/neurips2025/"><img src="https://coginterp.github.io/neurips2025/logo.png" width="60%" id="logo"></a>
        </div>
    </div>

    <div id="title-container">
        <div id="title-inner-container">
            <div class="supertitle">First Workshop on</div>
            <div class="title"><a href="https://coginterp.github.io/neurips2025/"><b>CogInterp</b>: Interpreting Cognition <br> in Deep Learning Models</a></div>
            <div class="subtitle">Dec. 6 or 7 @ NeurIPS 2025</div>
        </div>
    </div>

    <br>

    <div id="navigation">
        <ul>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/">About</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/call-for-papers">Call for Papers</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/schedule">Schedule</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/speakers">Speakers</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/organizers-reviewers">Organizers</a></strong></li>
        
      </ul>
    </div>

</section><section id="content">
        
    <h1 id="welcome">Welcome!</h1>
<p>Welcome to the first workshop on <strong>Interpreting Cognition in Deep Learning Models</strong>. The workshop will take place at <a href="https://neurips.cc/Conferences/2025"><strong>NeurIPS 2025</strong></a> in San Diego, USA on December 6 or 7, 2025.</p>
<!-- <div  style="text-align: center"><span class="alert"> We have extended the deadline until February 3, 2023, 23:59 AoE.</span></div> -->
<!-- <span class="alert">Most information is still preliminary and could change in the near future.</span> -->
<h2 id="about-the-workshop">About the workshop</h2>
<p>Following deep learning, multimodal machine learning has made steady progress, becoming ubiquitous in many domains. Learning representations from multiple modalities can be beneficial since different perceptual modalities can inform each other and ground abstract phenomena in a more robust, generalisable way. However, the complexity of different modalities can hinder the training process, requiring careful design of the model in order to learn meaningful representations. In light of these seemingly conflicting aspects of multimodal learning, we must improve our understanding of what makes each modality different, how they interact, and what are the desiderata of multimodal representations. With this workshop, we aim to bring the multimodal community together, promoting work on multimodal representation learning that provides systematic insights into the nature of the learned representations, as well as ways to improve and understand the training of multimodal models, both from a theoretical and empirical point of view.</p>
<h2 id="important-dates">Important Dates</h2>
<div id="dates" style="margin-bottom: 1em"></div>
<!-- <span class="alert">Preliminary dates, they are subject to change.</span>  -->
<ul>
<li><strong>Paper submission start:</strong> July 5, 2025  (midnight AoE)</li>
<li><strong>Paper submission deadline</strong>: August 22, 2025  (midnight AoE)</li>
<li><strong>Notification to authors:</strong> September 22, 2025  (midnight AoE)</li>
<li><strong>Camera-ready version:</strong> TBA</li>
<li><strong>Workshop date:</strong> December 6 or 7, 2025</li>
</ul>
<p>Contact us at <a href="mailto:coginterp@gmail.com">coginterp@gmail.com</a>. Specific dates are subject to change.</p>
<!-- ## With support from

  <div id="sponsor-logo-container">
      <div id="sponsor-inner-container">
          <img src="https://coginterp.github.io/neurips2025/googlelogo_color_416x140dp.png" width="25%" id="sponsor-logo">
      </div>
  </div>
 -->

    </section>
<div id="footer">
    Made with <a href="https://gohugo.io/">Hugo</a> and hosted on <a href="https://github.com/coginterp/neurips2025">GitHub</a>.
</div>


</body>

</html>