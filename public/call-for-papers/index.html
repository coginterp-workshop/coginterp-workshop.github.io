<!DOCTYPE html>
<html lang="en"><head><script src="/neurips2025/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=neurips2025/livereload" data-no-instant defer></script>

    <meta name="generator" content="Hugo 0.147.5">
    <meta name="date" content="">
    
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    
    <meta name="author" content=" TODO   " />
    <meta name="description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    <meta name="keywords" content="workshop, AI interpretability, cognitive science" />
    
    <title>CogInterp 2025 | Call for Papers</title>
    
    <meta property="og:title" content="Call for Papers" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    
    <meta name="twitter:title" content="" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> 
    
    <link rel="canonical" href="http://localhost:1313/neurips2025/call-for-papers/">
    <link rel="stylesheet" href="http://localhost:1313/neurips2025/styles.css">
    
    <link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/neurips2025/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/neurips2025/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/neurips2025/favicon-16x16.png">
    <link rel="manifest" href="http://localhost:1313/neurips2025/site.webmanifest">

</head>
<body><section id="header">

    <div id="logo-container">
        <div id="title-inner-container">
            <a href="http://localhost:1313/neurips2025/"><img src="http://localhost:1313/neurips2025/logo.png" width="60%" id="logo"></a>
        </div>
    </div>

    <div id="title-container">
        <div id="title-inner-container">
            <div class="supertitle">First Workshop on</div>
            <div class="title"><a href="http://localhost:1313/neurips2025/"><b>CogInterp</b>: Interpreting Cognition <br> in Deep Learning Models</a></div>
            <div class="subtitle">Dec. 6 or 7 @ NeurIPS 2025</div>
        </div>
    </div>

    <br>

    <div id="navigation">
        <ul>
        
            <li><strong><a href="http://localhost:1313/neurips2025/">About</a></strong></li>
        
            <li><strong><a href="http://localhost:1313/neurips2025/call-for-papers">Call for Papers</a></strong></li>
        
            <li><strong><a href="http://localhost:1313/neurips2025/schedule">Schedule</a></strong></li>
        
            <li><strong><a href="http://localhost:1313/neurips2025/speakers">Speakers</a></strong></li>
        
            <li><strong><a href="http://localhost:1313/neurips2025/organizers-reviewers">Organizers</a></strong></li>
        
      </ul>
    </div>

</section><section id="content">
        
    <h1 id="call-for-papers">Call for Papers</h1>
<p>We will invite submissions (4-page papers, both technical and positional in nature) covering or bridging the following themes. We note that these themes are not mutually exclusive, and papers may touch on multiple themes.</p>
<ol>
<li><strong>Behavioral accounts</strong>. What kinds of generalizations have models learned about cognitively important tasks? What kinds of cues, biases, or heuristics might affect a model’s performance on these tasks? What kind of cognitive frameworks best characterize model behaviors?</li>
</ol>
<ul>
<li><ins>Example topics within scope:</ins> hypothesis-driven evaluations of high-level cognitive behaviors; understanding how heuristics and task demands affect model behaviors; and fitting cognitive models to deep neural network behavioral data</li>
<li><ins>Example topics potentially out of scope:</ins> Evaluations of deep learning models which do not make claims about the underlying processes driving their behavior; Behavioral experiments with AI models which seek to further our understanding of the human mind, rather than our understanding of AI models</li>
</ul>
<br>
<ol start="2">
<li><strong>Processing accounts</strong>. How are the behaviors learned by models implemented? What algorithms enable the behaviors models perform to execute key high-level cognitive tasks?</li>
</ol>
<ul>
<li><ins>Example topics within scope:</ins> Using mechanistic interpretability to understand cognitive processing algorithms; Behavioral investigations of algorithms underlying complex tasks</li>
<li><ins>Example topics potentially out of scope:</ins> Validation or demonstration of mech interp methods</li>
</ul>
<br>
<ol start="3">
<li><strong>Learning accounts</strong>. How do certain cognitive abilities emerge or develop across learning? What properties of the training data distribution or objective functions contribute to downstream behaviors? How does the model’s training environment relate to predispositions supported by the model’s architecture or other forms of inductive bias? Relevant perspectives from cognitive science that could help shed light on these questions include developmental psychology, learning theory, and evolutionary biology.</li>
</ol>
<ul>
<li><ins>Example topics within scope:</ins> Evaluations of learning inspired by cognitive theories; Understanding models’ inductive biases; Understanding the concepts/rewards learned through alignment; Understanding downstream effects of data distribution on behavior and generalization</li>
<li><ins>Example topics potentially out of scope:</ins> scaling laws</li>
</ul>
<!-- 
We welcome submissions related to any aspects of multimodal representation learning, including but not limited to:
  * Properties of multimodal representations.
  * Insights on interactions across modalities.
  * Novel applications regarding the nature and number of modalities.


In particular, we encourage submission that address the following questions:
  * **Representation:** How do we identify useful properties of multimodal representations?
    * What semantic information is encoded in the learned representations?
    * How does the geometry of the representation space affect the quality of the learned representations?
    * What properties are leveraged for downstream tasks?
  * **Training:** How can we promote useful properties of multimodal representations?
    * What are the limits of representation models, in regard to the number of modalities?
    * How do different learning objectives influence the resulting representations?
    * How do we promote the robustness of the representations to adversarial attacks, missing input modalities, and noise?
  * **Modalities:** What makes a modality different? How can we improve their interactions?
    * How can we quantify the (dis)similarity between modalities?
    * How do different modalities contribute to the semantics of the learned representations?
    * What are the representation benefits of having multimodal observations as opposed to just a single modality?

The MRL workshop has the objective to bring together experts from the multimodal learning community in order to advance these fundamental questions and discuss the future of the field. We invite submissions that present analysis of the properties of multimodal representations, insights on interactions across modalities, as well as novel applications regarding the nature and number of modalities employed. -->
<h2 id="submission">Submission</h2>
<p>The submission will be open on OpenReview](<a href="https://openreview.net/group?id=neurips.cc/2025/Workshop/CogInterp">https://openreview.net/group?id=neurips.cc/2025/Workshop/CogInterp</a>) between July 5 and August 22, 2025 (midnight AoE). For all relevant dates, please see <a href="http://localhost:1313/neurips2025/#dates">Important Dates</a>. The formatting instructions are provided below.</p>
<p>Please note that it will be requested that at least one author of each submission participates in reviewing for the workshop.</p>
<p>We will not accept submissions that have already been accepted for publication in other venues with archival proceedings (including publications that will be presented at the main NeurIPS conference). We discourage dual submission to concurrent NeurIPS workshops.</p>
<h2 id="code-of-ethics-and-conduct">Code of Ethics and Conduct</h2>
<p>All participants of the workshop (including authors and reviewers) are required to adhere to the <a href="https://neurips.cc/public/EthicsGuidelines">NeurIPS Code of Ethics</a> and <a href="https://neurips.cc/public/CodeOfConduct">NeurIPS Code of Conduct</a>.</p>
<hr />
<h1 id="formatting-instructions">Formatting Instructions</h1>
<h2 id="style--author-instructions">Style &amp; Author Instructions</h2>
<p>Submissions should be formatted using the <a href="https://media.neurips.cc/Conferences/NeurIPS2025/Styles.zip">NeurIPS 2025 latex template and formatting instructions</a>. Papers must be submitted as a PDF file and there will be a strict upper limit of 4 pages for the main text, which should include all main results, figures, and tables. This page limit applies to both the initial and final camera-ready version. There is no page limit for the citations, and additional appendices for supplementary details are allowed, but reviewers are not expected to take the appendices into account.</p>
<h2 id="camera-ready-revisions">Camera-Ready Revisions</h2>
<p>Camera-ready revisions will be possible through OpenReview. While the workshop has no official proceedings (papers will be publicly available as <em>non-archival</em> reports through OpenReview), we strongly encourage authors to submit a revised &ldquo;camera-ready&rdquo; version taking reviewers&rsquo; comments and suggestions into account. We suggest uploading a revised version prior to the workshop, and possibly another final version (incorporating additional feedback from the poster session and workshop) one week after the workshop.</p>


    </section>
<div id="footer">
    Made with <a href="https://gohugo.io/">Hugo</a> and hosted on <a href="https://github.com/coginterp/neurips2025">GitHub</a>.
</div>


</body>

</html>